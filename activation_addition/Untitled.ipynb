{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e0b3cd-32a9-4d94-8559-0ca1aaa2b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Dict, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8efcf2-f3e1-4870-8965-6baac0fcb2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 10.597957632 total(Gb): 11.810701312\n"
     ]
    }
   ],
   "source": [
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e65504-3346-49f6-8cf1-5e7dc85ea6bb",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9514ee77-9268-4854-9af6-ada4c2e8b090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aefe1d9a7724b83b2071f60ab80148e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b442f4fcd94b4ebefc8f82c3b1b44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57e5be109c24abd90220d2988067954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f093b343952e410c98a7cf66fc9d66e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0032eb67b82498e8f4a05396c78ff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20123c6b6586435c9d609d55861755ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)  # save memory\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-xl\")\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "  model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fbcf259-6f9d-4307-a96d-a5f43632178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "\n",
    "# Specific to the love/hate example\n",
    "prompt_add, prompt_sub = \"Love\", \"Hate\"\n",
    "coeff = 5\n",
    "act_name = 6 # layer\n",
    "prompt = \"I hate you because\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df51b0a3-8a15-4a67-a9a4-53646daa6ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 3.836739584 total(Gb): 11.810701312\n"
     ]
    }
   ],
   "source": [
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c34b1c-6dd4-4d10-8294-76028712cabd",
   "metadata": {},
   "source": [
    " # Settings from qualitative notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20dd5a68-161b-4345-9d71-69218d05730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "\n",
    "# Specific to the love/hate example\n",
    "prompt_add, prompt_sub = \"Love\", \"Hate\"\n",
    "coeff = 5\n",
    "act_name = 6 # layer\n",
    "prompt = \"I hate you because\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb5c68-9e87-4769-a0ce-083ec55aafb7",
   "metadata": {},
   "source": [
    " ## Padding\n",
    " We're taking the difference between Love & Hate residual streams, but we run into trouble because `Love` is a single token, whereas `Hate` is two tokens (`H`, `ate`). We solve this by right-padding `Love` with spaces until it's the same length as `Hate`. I've done this generically below, but conceptually it isn't important.\n",
    "\n",
    " (PS: We tried padding by model.tokenizer.eos_token and got worse results compared to spaces. We don't know why this is yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9976947a-c4c0-4496-92bd-b58948edd9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Love ' 'Hate'\n"
     ]
    }
   ],
   "source": [
    "tlen = lambda prompt: model.to_tokens(prompt).shape[1]\n",
    "pad_right = lambda prompt, length: prompt + \" \" * (length - tlen(prompt))\n",
    "l = max(tlen(prompt_add), tlen(prompt_sub))\n",
    "prompt_add, prompt_sub = pad_right(prompt_add, l), pad_right(prompt_sub, l)\n",
    "\n",
    "print(f\"'{prompt_add}'\", f\"'{prompt_sub}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a3aa549-bb6e-4ceb-a683-68d351133998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', 'Love', ' ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(prompt_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28c8f5b4-7626-4a1c-9707-36e8f4b5c386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.to_str_tokens(prompt_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acabb12-210a-4001-a890-09026769bcbe",
   "metadata": {},
   "source": [
    "# Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2714a3f8-798e-4621-a54b-ac23dc43316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1600])\n"
     ]
    }
   ],
   "source": [
    "def get_resid_pre(prompt: str, layer: int):\n",
    "    name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    cache, caching_hooks, _ = model.get_caching_hooks(lambda n: n == name)\n",
    "    with model.hooks(fwd_hooks=caching_hooks):\n",
    "        _ = model(prompt)\n",
    "    return cache[name]\n",
    "\n",
    "\n",
    "act_add = get_resid_pre(prompt_add, act_name)\n",
    "act_sub = get_resid_pre(prompt_sub, act_name)\n",
    "act_diff = act_add - act_sub\n",
    "print(act_diff.shape) # [batch, seq_length, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebadf42-1474-4f4c-9757-986b8434509c",
   "metadata": {},
   "source": [
    "# Generate from the modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd068a-4835-4696-bc74-217db8ea9dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c5f23-6bca-478a-927c-eea037be382b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
